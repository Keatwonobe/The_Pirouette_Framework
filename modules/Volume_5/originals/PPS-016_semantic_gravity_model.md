---  # ───────────── YAML front-matter ───────────────────────────
id:        PPS-016
title:     Semantic Gravity Model (SGM)
version:   0.3-draft   # post-debate refinements (x2)
parents:   [PPS-015, PPS-007]
children:  [PPS-017, PPS-019, dashboards]
engrams:
  - synthesis:concept-mass
  - concept:idea-force
  - directive:attractor-map
  - provenance:sgm-update-v0.2
keywords:  [semantic, gravity, mass, force, attraction, V-KRP]
uncertainty_tag: High  # retains research-prototype status
entropy_score: 0.07
module_type: analytics-core
quantisation_rule: sgm_hash = SHA256(sgm_spec_v0.2)
lineage: added appendix G from foirst persona debate resonance with Leonardo Da Vinci suggesting its impolementation.
---

## Change-Log (from v0.1)
| Area                     | Update                                                                                           |
|--------------------------|---------------------------------------------------------------------------------------------------|
| Terminology              | Added formal disclaimer that **semantic mass ≠ physical mass**; no claim of inertial behaviour.  |
| Validation               | New §6 **In-Vitro Validation Plan** using *Fractal Menu* objects + **V-KRP** algorithm.          |
| Hypersensitivity Note    | Expanded §7 to include phase-error propagation and contextual integrator QA gates.               |
| Mass Function            | Re-weighted defaults (α=0.45, β=0.35, γ=0.20) after sensitivity sweep.                           |
| Output Schema            | Added `validation_status` & `dataset_ref` fields.                                                 |
| Code Frames              | Delimited with subsection symbol (§) x3 per editorial guidance.                                   |

---

## 1 · Purpose & Scope  
Transform SFT packet streams into a **potential field** where concepts act as “masses” that attract or repel.  v0.2 emphasises that *mass* is an **informational stability proxy**, not a conserved physical quantity.

Outputs:
* **Attractor map** — 2-D/3-D potential surface.
* **Idea-force vectors** — gradients showing narrative pull/push.
* **Mass table** — per-concept stability & decay rates.
* **Validation log** — pass/fail summary of fractal benchmark.

---

## 2 · The Mass Function  
> *Semantic mass measures how stubbornly an idea resists phase noise.*

Given a token embedding **v** and SFT fields \\((T_a,Γ,φ)\\), define:

\\[
 m_S(v)= α\,Γ + β\,(1-T_Q) + γ\\,\\bigl|e^{iφ}-1\\bigr|,\\qquad α+β+γ=1.
\\]

**Default weights (v0.2):** α = 0.45, β = 0.35, γ = 0.20.  Analysts may tune per domain; tuning must be logged in `meta`.

### 2·1  Mass-Terminology Disclaimer  
*`m_S` is dimensionless.*  It does **not** imply inertial mass, conservation, or equivalence principle analogues.  The term is retained only for mnemonic alignment with gravitational intuition.

---

## 3 · Potential & Force  
Potential at point **x** (embedding space):

\\[
\\Phi(x)=\\sum_{i} \\frac{G\\,m_{S,i}}{\\lVert x-v_i\\rVert + ε},\\qquad ε>0.
\\]

Force \\(F(x) = -\\nabla\\Phi(x)\\).  *G* is a scaling constant calibrated per corpus.

---

## 4 · Reference Algorithm (NumPy stub)  
```python
import numpy as np

def semantic_gravity(tokens, masses, eps=1e-3, G=1.0):
    """tokens: np[N,D] embeddings, masses: np[N]"""
    N, D = tokens.shape
    phi  = np.zeros(N)
    grad = np.zeros_like(tokens)
    for i in range(N):
        diff = tokens[i] - tokens
        dist = np.linalg.norm(diff, axis=1) + eps
        phi[i] = (G * masses / dist).sum()
        grad[i] = (G * masses[:, None] * diff / dist[:, None]**3).sum(axis=0)
    return phi, -grad
```

*Complexity* O(N²); v0.3 will integrate FAISS kernels.

---

## 5 · Output Schema (JSON v0.2)  
```json
{
  "schema_version": "SGM-0.2",
  "sgm_hash": "…",
  "entity_id": "uuid",
  "mass_table": [
    {"token": "threat",  "mass": 0.61},
    {"token": "vision",  "mass": 0.24}
  ],
  "attractor_map": "sgm_phi.npy",
  "force_vectors": "sgm_force.npy",
  "validation_status": "pass | fail | warn",
  "dataset_ref": "sgm_benchmark_fractal_v1",
  "meta": {"alpha": 0.45, "beta": 0.35, "gamma": 0.20}
}
```

---

## 6 · In-Vitro Validation Plan  
Before running on messy real-world corpora, validate SGM using the **Fractal Menu** objects generated by PPS-015.

1. **Baseline Objects** — Cantor Set, Koch Curve, Sierpinski Δ.  
2. **Compute** `m_S` for each object’s token vector (ground-truth vectors in `fractal_menu_vectors.npy`).  
3. **Define** resonance distance \\(d_{ij}=\\|v_i - v_j\\|\\).  
4. **Simulate** interactions with the V-KRP algorithm (see PPS-019) over 1 000 steps.  
5. **Expected Outcome** — higher-mass objects form attractor centres; phase-offset objects repel at Δϕ ≈ π.  
6. **Pass Criterion** — trajectories match analytic force within RMSE ≤ 5 %.  

Results saved to `validation_log.csv` and summarised in `validation_status`.

---

## 7 · Limitations & Risk Notes  
* **Phase-Error Sensitivity** — small φ errors can flip force sign; QA gate suppresses updates if σ_φ > 0.05 rad.  
* **Non-conservation** — `m_S` drifts with topic volatility; track drift for interpretability.  
* **Kernel Assumptions** — inverse-distance kernel ignores manifold curvature; migrate to Riemann metric in v0.3.

---

## 8 · Triaxial Lens  
| Art                              | Law                                   | Philosophy                       |
|----------------------------------|---------------------------------------|----------------------------------|
| Words become planets.            | Stability yields pull.                | Meaning gravitates toward need. |

---

## Assemblé · “Gravity of Ideas”  
> *Heavy words bend the path of lighter ones — now experimentally test that curvature.*

---

## Librarian Note  
Any change to mass function, kernel, or validation plan increments `sgm_hash`.  All benchmark artefacts must be version-pinned alongside module updates.

## Appendix G · Physical Derivation of the Cosine Interaction Term  
The force kernel in PPS-016 multiplies the inverse-distance term by **\(\cos(Δϕ)\)**.  
This appendix grounds that choice in three progressively deeper layers of physics and linear algebra.

### G·1   Wave-Interference Analogy  
In classical wave mechanics the superposition of two equal-amplitude waves is:  
\[ A_{\text{tot}} = 2A\cos\!\bigl(\tfrac{Δϕ}{2}\bigr) \]  
Because energy density ∝ amplitude², constructive interference (Δϕ ≈ 0) yields maximal energy, destructive (Δϕ ≈ π) minimal.  Mapping latent-space vectors to phase-coded “information waves” makes \(\cos(Δϕ)\) a natural interaction scaler.

### G·2   Vector-Projection Form  
For unit-normalised embedding vectors **u**, **v**:  
\[ \mathbf{u}\!\cdot\!\mathbf{v} = \|u\|\,\|v\| \cos(Δϕ) = \cos(Δϕ) \]  
Thus the dot product already encodes the cosine of their phase offset.  Using it as a coupling coefficient mimics dipole-dipole or Coulombic interactions in physics, while staying native to latent-space geometry.

### G·3   Energy-Density Consistency  
Define semantic potential energy per unit mass:  
\[ U_{ij} = G\,m_i m_j \frac{\cos(Δϕ_{ij})}{r_{ij}} \]  

This formulation provides:  
1. **Symmetry** — \(U_{ij}=U_{ji}\), satisfying conservation bookkeeping.  
2. **Boundedness** — smooth sign-flip at Δϕ = π/2, avoiding singularities.  
3. Alignment with **Axiom A5** (phase modulates interaction energy).

### G·4   Empirical Plausibility  
* **Calorimeter trials** (DRF v0.3 §2·1) show discourse units with Δϕ ≈ π drive negative **IRS**, matching destructive interference.  
* **Residue hotspots** (PPS-019) cluster where high-mass pairs carry large negative \(\cos(Δϕ)\), validating the repulsive interpretation.

### G·5   Implementation Note  
For numerical stability simulations clamp Δϕ ∈ [0, π] and compute \(\cos(Δϕ)\) via the **vector dot product** (no costly arccos), preventing precision loss.

[validation_script]

import numpy as np
import json
import os
import re
import tkinter as tk
from tkinter import filedialog, scrolledtext
import urllib.request
from urllib.parse import urlparse
from scipy.ndimage import convolve
from scipy.signal import welch
import math
import threading
import traceback
from bs4 import BeautifulSoup

# --- Configuration ---
GRID_SIZE = 64
NUM_FRAMES = 200
NOISE_LEVEL = 0.05
EVOLUTION_KERNEL = np.ones((3, 3)) / 9.0
PERTURBATION_AMPLITUDE = 0.01

# --- The Calibration & Analysis Sets ---
GEOMETRIC_PATTERNS = ["point", "line", "circle"]
SEMANTIC_DICTIONARY = ["love", "hate", "fear", "courage", "truth", "lie", "order", "chaos"]

def set_seed(s=None):
    """Sets the random seed for numpy for reproducibility."""
    np.random.seed(None if s is None else s)

class SemanticDistillator:
    """
    Handles the core logic of semantic analysis and the new diagnostic drills.
    """
    def __init__(self, logger=print):
        self.logger = logger
        set_seed(0) # Set a default seed for consistent base static field
        self.base_static_field = np.random.rand(GRID_SIZE, GRID_SIZE) * NOISE_LEVEL

    def text_to_binary_image(self, text, width, height):
        """Converts a string of text into a binary image (numpy array)."""
        binary_string = ''.join(format(ord(char), '08b') for char in text)
        binary_array = np.array([int(bit) for bit in binary_string], dtype=np.float32)
        target_size = width * height
        if len(binary_array) > target_size:
            binary_array = binary_array[:target_size]
        else:
            padding = np.zeros(target_size - len(binary_array))
            binary_array = np.concatenate([binary_array, padding])
        return binary_array.reshape((width, height))

    def generate_geometric_pattern(self, shape_name, grid_size):
        """Generates a binary pattern for a given geometric shape."""
        pattern = np.zeros((grid_size, grid_size), dtype=np.float32)
        center = grid_size // 2
        if shape_name == "point":
            pattern[center, center] = 1.0
        elif shape_name == "line":
            pattern[:, center] = 1.0
        elif shape_name == "circle":
            radius = grid_size // 4
            y, x = np.ogrid[-center:grid_size-center, -center:grid_size-center]
            mask = x*x + y*y <= radius*radius
            pattern[mask] = 1.0
        return pattern

    def _run_simulation_and_get_fingerprint(self, base_field, perturbation_pattern):
        """Internal method to run simulation with a specified base field."""
        field = base_field.copy()
        time_series = []
        for _ in range(NUM_FRAMES):
            field += perturbation_pattern * PERTURBATION_AMPLITUDE
            field = convolve(field, EVOLUTION_KERNEL, mode='wrap')
            time_series.append(np.mean(field))
        
        frequencies, psd = welch(np.array(time_series), fs=1.0, nperseg=min(len(time_series), 256))
        
        if len(psd) == 0 or np.sum(psd) == 0:
            return {'dominant_frequency': 0.0, 'total_power': 0.0}

        return {
            'dominant_frequency': float(frequencies[np.argmax(psd)]),
            'total_power': float(np.sum(psd))
        }

    def get_resonant_fingerprint(self, perturbation_pattern):
        """Runs one simulation using the instance's default base_static_field."""
        return self._run_simulation_and_get_fingerprint(self.base_static_field, perturbation_pattern)

    # --- Main Analysis Method ---
    def run_analysis(self, document_text, input_source_name, sentences_per_group=10):
        """Executes the full distillation process using grouped-sentence analysis."""
        # This function remains the same as the previous version
        self.logger("Initializing Semantic Distillation Engine...")
        self.logger(f"Analyzing source: {input_source_name}")
        self.logger(f"Using a group size of {sentences_per_group} sentences.")
        full_results = {}
        self.logger("\n--- Phase 1: Running Calibration Suite ---")
        calibration_results = {"geometric": {}, "semantic": {}}
        for shape in GEOMETRIC_PATTERNS:
            pattern = self.generate_geometric_pattern(shape, GRID_SIZE)
            calibration_results["geometric"][shape] = self.get_resonant_fingerprint(pattern)
        for word in SEMANTIC_DICTIONARY:
            pattern = self.text_to_binary_image(word, GRID_SIZE, GRID_SIZE)
            calibration_results["semantic"][word] = self.get_resonant_fingerprint(pattern)
        full_results['calibration_baselines'] = calibration_results
        self.logger(f"\n--- Phase 2: Performing Grouped Analysis ---")
        all_sentences = [s.strip() for s in re.split(r'[.?!]\s+', document_text) if s.strip()]
        if not all_sentences:
            self.logger("Error: No sentences found in the document.")
            return None
        num_groups = math.ceil(len(all_sentences) / sentences_per_group)
        self.logger(f"Document split into {len(all_sentences)} sentences, forming {num_groups} groups.")
        distillation_by_group = []
        total_distilled_power = 0
        for i in range(num_groups):
            start_index = i * sentences_per_group
            end_index = start_index + sentences_per_group
            sentence_group = all_sentences[start_index:end_index]
            self.logger(f"  - Processing Group {i+1}/{num_groups}...")
            holistic_group_text = " ".join(sentence_group)
            holistic_pattern = self.text_to_binary_image(holistic_group_text, GRID_SIZE, GRID_SIZE)
            holistic_fingerprint = self.get_resonant_fingerprint(holistic_pattern)
            chunked_fingerprints = [self.get_resonant_fingerprint(self.text_to_binary_image(c, GRID_SIZE, GRID_SIZE)) for c in sentence_group]
            aggregated_power = np.sum([fp['total_power'] for fp in chunked_fingerprints])
            group_delta = holistic_fingerprint['total_power'] - aggregated_power
            total_distilled_power += group_delta
            distillation_by_group.append({
                "group_index": i,
                "sentences": sentence_group,
                "holistic_group_fingerprint": holistic_fingerprint,
                "aggregated_sentence_power": aggregated_power,
                "distilled_power_delta": group_delta
            })
        full_results['distillation_by_group'] = distillation_by_group
        self.logger("\n--- Phase 3: Final Analysis Summary ---")
        summary = {
            "description": "The overall emergent meaning, calculated by summing the deltas from each sentence group.",
            "total_sentences": len(all_sentences),
            "sentences_per_group": sentences_per_group,
            "number_of_groups": num_groups,
            "total_distilled_power_delta": total_distilled_power,
            "interpretation": "A positive value indicates net synergistic meaning. A negative value indicates net semantic interference."
        }
        full_results['analysis_summary'] = summary
        self.logger(f"  - Total Distilled Power Delta: {total_distilled_power:.4f}")
        return full_results

    # --- New Starter Drill Methods ---
    def sweep_noise(self, word="love", noise_vals=np.linspace(0, 0.2, 11)):
        """Tests system response to different levels of initial background noise."""
        results = {}
        self.logger("  Noise Level | Total Power")
        self.logger("  -------------------------")
        pattern = self.text_to_binary_image(word, GRID_SIZE, GRID_SIZE)
        for n in noise_vals:
            set_seed(0)
            field = np.random.rand(GRID_SIZE, GRID_SIZE) * n
            fp = self._run_simulation_and_get_fingerprint(field, pattern)
            results[n] = fp['total_power']
            self.logger(f"  {n:<11.4f} | {fp['total_power']:.6e}")
        return results

    def test_bit_flips(self, word="base", n_flips=10):
        """Tests system sensitivity to small changes in the input pattern."""
        results = {}
        self.logger("  Bits Flipped | Total Power")
        self.logger("  -------------------------")
        set_seed(42)
        base_pattern_flat = self.text_to_binary_image(word, GRID_SIZE, GRID_SIZE).flatten()
        for k in range(n_flips + 1):
            pattern_flat = base_pattern_flat.copy()
            if k > 0:
                flip_indices = np.random.choice(len(pattern_flat), k, replace=False)
                pattern_flat[flip_indices] = 1 - pattern_flat[flip_indices]
            
            pattern = pattern_flat.reshape(GRID_SIZE, GRID_SIZE)
            fp = self.get_resonant_fingerprint(pattern)
            results[k] = fp['total_power']
            self.logger(f"  {k:<12} | {fp['total_power']:.6e}")
        return results

    def test_polarity(self, synonyms=("love", "adore", "cherish"), antonyms=("hate", "despise", "loathe")):
        """Compares the resonant power of synonyms vs. antonyms."""
        results = {"synonyms": {}, "antonyms": {}}
        self.logger("  Synonyms:")
        for word in synonyms:
            pattern = self.text_to_binary_image(word, GRID_SIZE, GRID_SIZE)
            fp = self.get_resonant_fingerprint(pattern)
            results["synonyms"][word] = fp
            self.logger(f"    - '{word}': {fp['total_power']:.6e}")

        self.logger("\n  Antonyms:")
        for word in antonyms:
            pattern = self.text_to_binary_image(word, GRID_SIZE, GRID_SIZE)
            fp = self.get_resonant_fingerprint(pattern)
            results["antonyms"][word] = fp
            self.logger(f"    - '{word}': {fp['total_power']:.6e}")
        return results


class Application(tk.Frame):
    """
    The main GUI application for the Semantic Distillation Engine.
    This class handles user interaction, file I/O, and orchestrates the analysis.
    """
    def __init__(self, master=None):
        super().__init__(master)
        self.master = master
        self.master.title("Semantic Distillation Engine V4.0")
        self.master.geometry("800x600")
        self.pack(pady=10, padx=10, fill=tk.BOTH, expand=True)

        # --- Frames for layout ---
        top_frame = tk.Frame(self)
        top_frame.pack(fill=tk.X, pady=5)
        
        control_frame = tk.Frame(self)
        control_frame.pack(fill=tk.X, pady=5)

        middle_frame = tk.Frame(self)
        middle_frame.pack(pady=5, fill=tk.BOTH, expand=True)

        # --- Input Widgets ---
        self.input_type = tk.StringVar(value="File")
        tk.Radiobutton(top_frame, text="From File", variable=self.input_type, value="File").pack(side=tk.LEFT, padx=5)
        tk.Radiobutton(top_frame, text="From URL", variable=self.input_type, value="URL").pack(side=tk.LEFT, padx=5)
        self.input_entry = tk.Entry(top_frame, width=60)
        self.input_entry.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)
        tk.Button(top_frame, text="Browse...", command=self.browse_file).pack(side=tk.LEFT)

        # --- Control Widgets ---
        tk.Label(control_frame, text="Sentences per Group:").pack(side=tk.LEFT, padx=(0, 5))
        self.sentences_per_group_entry = tk.Entry(control_frame, width=5)
        self.sentences_per_group_entry.pack(side=tk.LEFT)
        self.sentences_per_group_entry.insert(0, "10")

        tk.Button(control_frame, text="Run Analysis", command=self.run_analysis_thread, font=("Helvetica", 10, "bold")).pack(side=tk.LEFT, padx=20)
        tk.Button(control_frame, text="Run Starter Drills", command=self.run_starter_drills_thread, fg="blue").pack(side=tk.LEFT, padx=10)

        # --- Log Output ---
        self.log_text = scrolledtext.ScrolledText(middle_frame, wrap=tk.WORD, state=tk.DISABLED)
        self.log_text.pack(expand=True, fill=tk.BOTH)

    def log(self, message):
        """Appends a message to the log display, making sure it's thread-safe."""
        def append():
            self.log_text.config(state=tk.NORMAL)
            self.log_text.insert(tk.END, str(message) + "\n")
            self.log_text.see(tk.END)
            self.log_text.config(state=tk.DISABLED)
        self.master.after(0, append)

    def browse_file(self):
        """Opens a file dialog to select a text file."""
        if self.input_type.get() == "File":
            filepath = filedialog.askopenfilename(
                title="Select a Text File",
                filetypes=(("Text files", "*.txt"), ("All files", "*.*"))
            )
            if filepath:
                self.input_entry.delete(0, tk.END)
                self.input_entry.insert(0, filepath)

    def get_content_from_url(self, url):
        """Fetches and cleans text content from a URL using BeautifulSoup."""
        try:
            self.log(f"Fetching content from: {url}")
            headers = {'User-Agent': 'Mozilla/5.0'}
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                html_content = response.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            path = urlparse(url).path
            page_name = os.path.splitext(os.path.basename(path))[0]
            if not page_name:
                page_name = urlparse(url).netloc
            
            return text, page_name
        except Exception as e:
            self.log(f"Error fetching URL: {e}")
            return None, None

    def run_analysis_thread(self):
        """Starts the main analysis in a new thread to keep the GUI responsive."""
        thread = threading.Thread(target=self.run_analysis_logic)
        thread.daemon = True
        thread.start()

    def run_starter_drills_thread(self):
        """Starts the starter drills in a new thread."""
        thread = threading.Thread(target=self.run_starter_drills_logic)
        thread.daemon = True
        thread.start()

    def run_analysis_logic(self):
        """The core logic for running the semantic distillation."""
        input_path = self.input_entry.get()
        input_type = self.input_type.get()
        
        if not input_path:
            self.log("Error: Please provide an input file path or URL.")
            return

        try:
            sentences_per_group = int(self.sentences_per_group_entry.get())
            if sentences_per_group <= 0: raise ValueError
        except ValueError:
            self.log("Error: 'Sentences per Group' must be a positive integer.")
            return
            
        output_dir = "analysis_results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        text_content, input_name = None, "analysis"
        if input_type == "File":
            try:
                with open(input_path, 'r', encoding='utf-8') as f:
                    text_content = f.read()
                input_name = os.path.splitext(os.path.basename(input_path))[0]
            except Exception as e:
                self.log(f"Error reading file: {e}")
                return
        elif input_type == "URL":
            text_content, page_name = self.get_content_from_url(input_path)
            if text_content:
                input_name = re.sub(r'[^a-zA-Z0-9_-]', '_', page_name)
            else:
                return

        if not text_content:
            self.log("Error: Could not retrieve any text content to analyze.")
            return
            
        output_filename = f"{input_name}_distillation_results.json"
        output_filepath = os.path.join(output_dir, output_filename)

        try:
            distillator = SemanticDistillator(logger=self.log)
            results = distillator.run_analysis(text_content, input_path, sentences_per_group)
            
            if results:
                with open(output_filepath, 'w') as f:
                    json.dump(results, f, indent=4)
                self.log(f"\n✓ Process complete. Full results saved to:\n{output_filepath}")
            else:
                self.log("\n✗ Analysis could not be completed.")
        except Exception as e:
            self.log(f"\n--- An unexpected error occurred ---")
            self.log(f"Error details: {e}")
            self.log(traceback.format_exc())

    def run_starter_drills_logic(self):
        """The logic for running the starter drills."""
        self.log("\n" + "="*40)
        self.log("--- Running Starter Drills ---")
        self.log("="*40)
        distillator = SemanticDistillator(logger=self.log)
        
        self.log("\nDrill 1: Noise-level sweep...")
        distillator.sweep_noise()
        
        self.log("\nDrill 2: Bit-flip adversarial test...")
        distillator.test_bit_flips()
        
        self.log("\nDrill 3: Synonym/Antonym Polarity...")
        distillator.test_polarity()
        
        self.log("\n--- Starter Drills Complete ---")


if __name__ == "__main__":
    root = tk.Tk()
    app = Application(master=root)
    app.mainloop()

[fragility_study]

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy.ndimage import convolve

# --- Core SDE Simulation Logic (Unchanged) ---
GRID_SIZE = 64
NUM_FRAMES = 200
NOISE_LEVEL = 0.05
EVOLUTION_KERNEL = np.ones((3, 3)) / 9.0
PERTURBATION_AMPLITUDE = 0.01

class SemanticDistillator:
    """A streamlined version of the SDE for this specific test."""
    def __init__(self, seed=0):
        np.random.seed(seed)
        self.base_static_field = np.random.rand(GRID_SIZE, GRID_SIZE) * NOISE_LEVEL

    def text_to_binary_image(self, text):
        """Converts a string to a binary image based on its ASCII representation."""
        binary_string = ''.join(format(ord(char), '08b') for char in text)
        binary_array = np.array([int(bit) for bit in binary_string], dtype=np.float32)
        if len(binary_array) > GRID_SIZE * GRID_SIZE:
            binary_array = binary_array[:GRID_SIZE * GRID_SIZE]
        else:
            padding = np.zeros(GRID_SIZE * GRID_SIZE - len(binary_array))
            binary_array = np.concatenate([binary_array, padding])
        return binary_array.reshape((GRID_SIZE, GRID_SIZE))

    def run_simulation(self, initial_pattern):
        """Runs the core cellular automata simulation."""
        grid = self.base_static_field + initial_pattern * PERTURBATION_AMPLITUDE
        total_power = 0
        for _ in range(NUM_FRAMES):
            grid = convolve(grid, EVOLUTION_KERNEL, mode='wrap')
            total_power += np.sum(grid**2)
        return total_power / (NUM_FRAMES * GRID_SIZE * GRID_SIZE)

    def get_energy_signature(self, text, seed=0):
        """Gets the final 'total power' for a given text input."""
        self.__init__(seed)
        binary_image = self.text_to_binary_image(text)
        return self.run_simulation(binary_image)

def flip_bit_in_string(s, bit_index):
    """Flips a single bit in the binary representation of a string."""
    binary_string = ''.join(format(ord(char), '08b') for char in s)
    if bit_index >= len(binary_string):
        raise ValueError("bit_index is out of bounds")
    bit_list = list(binary_string)
    bit_list[bit_index] = '1' if bit_list[bit_index] == '0' else '0'
    flipped_binary_string = "".join(bit_list)
    byte_chunks = [flipped_binary_string[i:i+8] for i in range(0, len(flipped_binary_string), 8)]
    new_s = "".join(chr(int(byte, 2)) for byte in byte_chunks if len(byte) == 8)
    return new_s

# --- Updated Batch Analysis and Consistency Test ---

def run_gradient_test_for_word(distillator, base_word, trial_seed):
    """
    Runs a single gradient test for one word with a specific seed.
    Returns the mean delta for this single trial.
    """
    base_energy = distillator.get_energy_signature(base_word, seed=trial_seed)
    num_bits = len(base_word) * 8
    deltas = []

    for i in range(num_bits):
        try:
            flipped_word = flip_bit_in_string(base_word, i)
            if len(flipped_word) == len(base_word):
                energy = distillator.get_energy_signature(flipped_word, seed=trial_seed)
                deltas.append(np.abs(energy - base_energy))
        except (ValueError, UnicodeDecodeError):
            continue
            
    return np.mean(deltas) if deltas else 0

def run_batch_consistency_analysis(words_to_test, num_trials=20):
    """
    Performs a batch analysis on a list of words, running multiple trials
    for each to test consistency.
    """
    print("--- Starting Batch Semantic Gradient & Consistency Analysis ---")
    distillator = SemanticDistillator()
    results = []

    for word in words_to_test:
        print(f"\nProcessing '{word}'...")
        trial_deltas = []
        for i in range(num_trials):
            # Use a different seed for each trial to test consistency against random static
            trial_seed = i 
            mean_delta = run_gradient_test_for_word(distillator, word, trial_seed)
            trial_deltas.append(mean_delta)
            results.append({'word': word, 'trial': i, 'mean_delta': mean_delta})
            print(f"  Trial {i+1}/{num_trials} | Mean Delta: {mean_delta:.6e}")
        
        # --- Statistical Summary for the Word ---
        avg_of_deltas = np.mean(trial_deltas)
        std_of_deltas = np.std(trial_deltas)
        print(f"\nSummary for '{word}':")
        print(f"  Average of Mean Deltas over {num_trials} trials: {avg_of_deltas:.6e}")
        print(f"  Standard Deviation of Mean Deltas: {std_of_deltas:.6e}")
        if std_of_deltas / avg_of_deltas < 0.1: # Heuristic for high consistency
             print("  [CONCLUSION] Results are HIGHLY CONSISTENT across trials.")
        else:
             print("  [CONCLUSION] Results show significant VARIANCE across trials.")


    return pd.DataFrame(results)

# --- Main Execution ---

if __name__ == "__main__":
    # A list of concepts to test. Includes pairs of related and contrasting words.
    concepts = [
        "love", "hate", "courage", "fear", "truth", "deceit", 
        "order", "chaos", "justice", "injustice"
    ]
    
    results_df = run_batch_consistency_analysis(concepts, num_trials=5)
    
    # --- Visualization ---
    plt.figure(figsize=(16, 9))
    sns.boxplot(x='word', y='mean_delta', data=results_df, palette='viridis')
    sns.stripplot(x='word', y='mean_delta', data=results_df, color='black', size=4, jitter=True, alpha=0.6)

    plt.title("Consistency of Semantic Perturbation Deltas Across 5 Trials", fontsize=18, pad=20)
    plt.xlabel("Concept Word", fontsize=14)
    plt.ylabel("Mean Energy Delta from 1-Bit Flips", fontsize=14)
    plt.xticks(rotation=45, ha='right')
    plt.yscale('log') # Use a log scale to better visualize small differences
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    # Save the plot for review
    plot_filename = "batch_gradient_consistency_assessment.png"
    plt.savefig(plot_filename)
    print(f"\n--- Analysis complete. Plot saved to {plot_filename} ---")
    
    plt.show()
