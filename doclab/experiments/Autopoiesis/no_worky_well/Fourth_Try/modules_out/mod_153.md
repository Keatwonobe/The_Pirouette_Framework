1. Pretrain on task loss (\mathcal L_{\rm task}) to stabilize.
2. Switch on (\mathcal L_{\text{pirouette}}) (curriculum on (\lambda)).
3. Periodically **refit PRG exponents** from the modelâ€™s own multiscale outputs (EM-style), then continue training.