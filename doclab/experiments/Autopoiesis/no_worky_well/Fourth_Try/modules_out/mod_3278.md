The following Python script represents the full implementation of the Semantic Gravity Analyzer. It includes the refined concept filtering, the content-based seeding, and the "test particle" simulation engine.

import numpy as np
import json
import argparse
import matplotlib.pyplot as plt
from collections import defaultdict
from pathlib import Path
import datetime
import hashlib
import re

def load_concepts_from_distillation(file_path: str) -> dict:
    """
    Loads concepts and their 'total_power' from a distillation results file.
    Averages the power for each unique word and applies filters to focus on semantic content.
    """
    print(f"Loading concepts from: {file_path}")
    word_powers = defaultdict(list)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        if 'distillation_by_group' not in data:
            print("Error: 'distillation_by_group' not found in JSON.")
            return {}

        stop_words = set(['the', 'is', 'are', 'a', 'an', 'in', 'to', 'of', 'and', 'for', 'was', 'were', 'it', 'that', 'as', 'by', 'with', 'from', 'at', 'he', 'she', 'they', 'pnbsp', 'ppnbsp'])

        for group in data['distillation_by_group']:
            holistic_power = group.get('holistic_group_fingerprint', {}).get('total_power', 0)
            words = " ".join(group.get('sentences', [])).lower().split()
            if not words: continue
            
            power_per_word = holistic_power / len(words) if len(words) > 0 else 0
            
            for word in set(words):
                clean_word = ''.join(filter(str.isalnum, word))
                if clean_word.isalpha() and len(clean_word) > 3 and clean_word not in stop_words:
                    word_powers[clean_word].append(power_per_word)

        if not word_powers:
            print("Warning: No valid concepts found after filtering.")
            return {}

        semantic_masses = {word: np.mean(powers) for word, powers in word_powers.items()}
        print(f"Successfully loaded and filtered {len(semantic_masses)} unique concepts.")
        return semantic_masses

    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"Error reading or parsing file: {e}")
        return {}

def run_gravity_analysis(concepts: dict, filename: str, grid_size: int = 50):
    if not concepts:
        return None, None, None, None, None

    seed = int(hashlib.sha256(filename.encode('utf-8')).hexdigest(), 16) % 10**8
    np.random.seed(seed)
    
    sorted_concepts = sorted(concepts.items(), key=lambda item: item[1], reverse=True)[:50]
    positions = np.random.rand(len(sorted_concepts), 2) * grid_size
    masses = np.array([m for _, m in sorted_concepts])
    labels = [name for name, _ in sorted_concepts]
    
    x = np.linspace(0, grid_size - 1, grid_size)
    y = np.linspace(0, grid_size - 1, grid_size)
    xx, yy = np.meshgrid(x, y)
    
    potential = np.zeros_like(xx)
    forces_x = np.zeros_like(xx)
    forces_y = np.zeros_like(yy)

    for pos, mass in zip(positions, masses):
        dx, dy = xx - pos[0], yy - pos[1]
        r_sq = dx**2 + dy**2 + 1e-6
        r = np.sqrt(r_sq)
        
        potential -= mass / r
        force_magnitude = mass / r_sq
        forces_x += force_magnitude * (dx / r)
        forces_y += force_magnitude * (dy / r)
        
    return potential, np.dstack((forces_x, forces_y)), positions, labels, masses

def simulate_test_particle_path(force_field, start_pos, grid_size, num_steps=500, learning_rate=1.0):
    """
    Simulates the path of a test particle through the force field.
    Increased num_steps and learning_rate for more significant movement.
    """
    path = [start_pos]
    current_pos = np.array(start_pos, dtype=float)
    
    for _ in range(num_steps):
        x_idx = int(np.clip(round(current_pos[0]), 0, grid_size - 1))
        y_idx = int(np.clip(round(current_pos[1]), 0, grid_size - 1))
        
        force_vector = force_field[y_idx, x_idx]
        
        # The position update is now more aggressive due to the higher learning_rate
        current_pos += learning_rate * force_vector
        path.append(current_pos.copy())
        
        # Stop if the particle's movement becomes negligible
        if np.linalg.norm(learning_rate * force_vector) < 1e-4:
            break
            
    return np.array(path)

def visualize_field(potential, forces, positions, labels, output_path, test_particle_paths=None):
    if potential is None: return
    fig, ax = plt.subplots(figsize=(14, 12))
    contour = ax.contourf(potential, levels=50, cmap='inferno')
    plt.colorbar(contour, ax=ax, label='Semantic Potential (Lower = Stronger Pull)')
    
    skip = 5
    force_magnitudes = np.sqrt(forces[::skip, ::skip, 0]**2 + forces[::skip, ::skip, 1]**2)
    scale_value = np.percentile(force_magnitudes[force_magnitudes > 0], 90) if np.any(force_magnitudes > 0) else 1
    ax.quiver(
        np.arange(0, forces.shape[1], skip),
        np.arange(0, forces.shape[0], skip),
        forces[::skip, ::skip, 0],
        forces[::skip, ::skip, 1],
        color='white', alpha=0.8,
        scale=scale_value * 25,
        width=0.003
    )
    
    ax.scatter(positions[:, 0], positions[:, 1], c='cyan', s=100, edgecolors='black', label='Concepts (Masses)')
    
    for i, label in enumerate(labels[:10]):
        ax.text(positions[i, 0], positions[i, 1] + 1, label, color='white', ha='center', fontsize=9)

    if test_particle_paths:
        colors = ['lime', 'magenta', 'yellow', 'orange']
        for i, (name, path) in enumerate(test_particle_paths.items()):
            color = colors[i % len(colors)]
            ax.plot(path[:, 0], path[:, 1], color=color, linewidth=2.5, label=f"Path: '{name}'")
            ax.scatter(path[0, 0], path[0, 1], color=color, s=150, marker='X', edgecolors='black')

    ax.set_title("Semantic Gravity Well & Force Field", fontsize=20)
    ax.set_xlabel("Conceptual Space X-Axis")
    ax.set_ylabel("Conceptual Space Y-Axis")
    ax.set_aspect('equal')
    ax.legend()
    plt.savefig(output_path, dpi=150)
    print(f"Visualization saved to {output_path}")
    plt.close()

def save_gravity_data(source_file, labels, masses, positions, output_path):
    data_to_save = { "analysis_metadata": { "source_file": source_file, "timestamp_utc": datetime.datetime.utcnow().isoformat(), "top_n_concepts": len(labels)}, "concepts": [] }
    for i in range(len(labels)):
        data_to_save["concepts"].append({ "label": labels[i], "semantic_mass": masses[i], "position": positions[i].tolist()})
    with open(output_path, 'w') as f: json.dump(data_to_save, f, indent=4)
    print(f"Gravity data saved to {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze Semantic Gravity from a distillation file.")
    parser.add_argument('--inputfile', required=True, help='Path to the semantic distillation JSON file.')
    args = parser.parse_args()

    concept_masses = load_concepts_from_distillation(args.inputfile)
    grid_size = 50
    potential_field, force_field, concept_positions, concept_labels, concept_mass_values = run_gravity_analysis(concept_masses, args.inputfile, grid_size)
    
    base_name = Path(args.inputfile).stem
    output_image_path = f"{base_name}_gravity_map_with_paths.png"
    output_json_path = f"{base_name}_gravity_data.json"

    particle_paths = {}
    if force_field is not None:
        test_particles = {
            "power": (10, 40),
            "freedom": (40, 40),
            "wisdom": (5, 5)
        }
        print("\nSimulating test particle trajectories...")
        for name, start_pos in test_particles.items():
            path = simulate_test_particle_path(force_field, start_pos, grid_size)
            particle_paths[name] = path
            print(f"  Path for '{name}' calculated, starting at {start_pos}.")

    if potential_field is not None:
        visualize_field(potential_field, force_field, concept_positions, concept_labels, output_image_path, particle_paths)

    if concept_labels:
        save_gravity_data(args.inputfile, concept_labels, concept_mass_values, concept_positions, output_json_path)