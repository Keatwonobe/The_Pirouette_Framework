For language, window tokens into scales (L\in{32,128,512}).
Compute (K_\tau(L)=\text{CE}*{\rm baseline}-\text{CE}*{\rm model}(L)).
Set (V_\Gamma(L)) to the variance of *masked exogenous features* (topic switches / speaker changes).
Fit (d_K,d_\Gamma,\zeta_{\cdot}).
Train with (\mathcal L_{\text{pirouette}}).
Expectation: better long-context loss and fewer incoherent jumps.